{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c4661b-97fe-4139-8625-185fcbd0e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installiamo le librerie necessarie\n",
    "!pip install --upgrade --quiet langchain\n",
    "!pip install --upgrade --quiet langchain_openai\n",
    "!pip install --upgrade --quiet langchain_chroma\n",
    "!pip install --upgrade --quiet langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504cc2ac-1f7e-4da2-9614-9478874ab0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carichiamo l'API Key di OpenAI\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94e09ac-40cf-48a5-9e99-44fc87e33a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diviso il testo in 271 documenti\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Usiamo il Document Loader per caricare il video\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"./i_promessi_sposi.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Il nostro documento è troppo grande per entrare nella context windows di GPT\n",
    "# Usiamo un text spliltter per dividerlo in documenti più piccoli\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=5000,\n",
    ")\n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(f\"Diviso il testo in {len(documents)} documenti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b24dd0c-5591-4639-9db4-0b2e3d23fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creiamo il vectorstore con i documenti...\n",
      "Creato il vectorstore con i documenti\n"
     ]
    }
   ],
   "source": [
    "# creiamo un vectorstore con i nostri documenti\n",
    "print(\"Creiamo il vectorstore con i documenti...\")\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "print(\"Creato il vectorstore con i documenti\")\n",
    "\n",
    "# Per rispondere ad una domanda dobbiamo trovare i documenti più rilevanti. Lo faremo cercando la similarità tra il testo della domanda e i documenti\n",
    "retriever = VectorStoreRetriever(vectorstore=db, search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# preparamo un prompt che usi la domanda e i documenti\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"\"\"\n",
    "Utilizza il contenuto del testo fornito per rispondere alla domanda dell'utente. Rispondi che non conosci la risposta se non è contenuta nel testo.\n",
    "Domanda: {question}\n",
    "\t \n",
    "Testo:\n",
    "{text}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# Carichiamo il modello di GPT\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "# componiamo la chain\n",
    "chain = ({\"text\": RunnablePassthrough(), \"question\": RunnablePassthrough()} | prompt_template | llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29e3a58-1857-4c48-ab0b-041be15b0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 10 documenti simili\n",
      "Don Abbondio era un curato, un uomo di chiesa, che non era nato con un cuore coraggioso. Fin dai suoi primi anni, aveva compreso che la peggior condizione, a quei tempi, era quella di un individuo senza mezzi di difesa, e che non si sentiva inclinato a essere divorato. La forza legale non proteggeva l'uomo tranquillo e inoffensivo, e don Abbondio aveva quindi sviluppato un sistema per evitare i contrasti e cedere in quelli che non poteva evitare. Era un uomo che cercava di mantenere la propria quiete, evitando di prendere parte a conflitti e cercando di stare dalla parte del più forte quando costretto a scegliere. Era anche un rigido censore degli uomini che non si comportavano come lui, ma solo quando poteva farlo senza pericolo.\n"
     ]
    }
   ],
   "source": [
    "# Inserisci una domanda relativa al contenuto del testo\n",
    "question = \"Chi era Don Abbondio?\"\n",
    "\n",
    "# troviamo i documenti simili\n",
    "similar_documents = retriever.invoke(question)\n",
    "print(f\"Trovati {len(similar_documents)} documenti simili\")\n",
    "\n",
    "# formattiamo i documenti\n",
    "text = \"\"\n",
    "for document in similar_documents:\n",
    "\ttext += document.page_content + \"\\n\"\n",
    "\t\n",
    "# Eseguiamo la chain\n",
    "answer = chain.invoke({\"text\": text, \"question\": question})\n",
    "\n",
    "# Stampiamo il risultato\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47517639-1ae2-4840-afc1-888004ec5cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
